{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro-To-TF (Part-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last two tutorials [Intro-To-TF-Part1](https://github.com/Praneet460/MLCC/tree/master/Day1) and [Intro-To-TF-Part2](https://github.com/Praneet460/MLCC/tree/master/Day2), we learn about the different mathematical operations we can perform with the tensors.\n",
    "And now, we see how we can create our First ```Neural Network```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Simple Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define neural networks in TensorFlow using computation graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./computation-graph-2.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph shown above takes an input (x) and computes an output (out). The other parameters are :\n",
    "1. <b>w</b> : weight, \n",
    "2. <b>x</b> : input,\n",
    "3. <b>b</b> : bias,\n",
    "4. <b>out</b> : output\n",
    "\n",
    "<b>Notice</b> : out = sigmoid(w*x+b), where sigmoid is an activation function, we can also use other activation function like :\n",
    "* out = tanh(z) \n",
    "* out = relu(z)\n",
    "<br>\n",
    "where,\n",
    "* Tanh - Hyperbolic tangent\n",
    "* ReLu - Rectified Linear Units\n",
    "<br>\n",
    "Just remember :\n",
    "<b><i>\"Input times weights, add Bias and Activate\"</i></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build our first Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Praneet\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# Import required library\n",
    "import tensorflow as tf\n",
    "\n",
    "# Defining Inputs\n",
    "x = tf.placeholder(dtype=tf.float32, shape=(None, 2), name=\"x\")\n",
    "w = tf.Variable(tf.ones((2, 1)), dtype=tf.float32)\n",
    "b = tf.Variable(tf.zeros(2), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>tf.Variable</b> : Variables in TensorFlow are in-memory buffers containing tensors which have to be explicitly initialized and used in-graph to maintain state across session. Variables are especially useful once you start with trining models, and they are used to hold and update parameters. Just like any <i>Tensor</i>, variables created with <i>Variable()</i> can be used as inputs for other Ops in the graph.\n",
    "The signature of the <i>tf.Variable</i> is :<br>\n",
    "tf.Variable(```<initial-value>```, name=```<optional-name>```)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define operation for z\n",
    "z = tf.add(tf.matmul(x, w), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using sigmoid as activation function \n",
    "out = tf.sigmoid(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.59868765\n"
     ]
    }
   ],
   "source": [
    "# Executing the computation graph\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    result = session.run(out, feed_dict={x: [[0.25, 0.15]]})\n",
    "    print(result[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```tf.global_variables_initializer()``` : Variable initializers must be run explicitly before other ops in your model can be run. The easiest way to do that is to add an op that runs all the variable initializers, and run that op before using the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.379949\n"
     ]
    }
   ],
   "source": [
    "# using tanh as activation function\n",
    "out_tanh = tf.tanh(z)\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    result = session.run(out_tanh, feed_dict={x: [[0.25, 0.15]]})\n",
    "    print(result[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n"
     ]
    }
   ],
   "source": [
    "# using relu as activation function\n",
    "out_relu = tf.nn.relu(z)\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    result = session.run(out_relu, feed_dict={x: [[0.25, 0.15]]})\n",
    "    print(result[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Conclusion</b> : Using three different activation functions we get three different results.And ReLu activation function provide us will more accurate result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
